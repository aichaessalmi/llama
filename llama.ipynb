{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.48.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torch in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.3.0+cu118)\n",
      "Requirement already satisfied: accelerate in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.34.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\pc user\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pc user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\PC USER\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Remplace \"TON_TOKEN_HF\" par ton vrai token Hugging Face\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AichaESSALMI1\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:823: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d5951848fc4af8be4ee2436c502cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC USER\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-7b-chat-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba876d286ac443368aaf2c72f0812ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439a651f201943a19e0ad9294263c287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd2409c269645a492408c4c7115d56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Cell 3: Dataset Loading Function\n",
    "def load_arabic_dataset(base_path):\n",
    "      \"\"\"Load Arabic text files from directories\"\"\"\n",
    "      texts = []\n",
    "      labels = []\n",
    "\n",
    "      print(\"Starting to load dataset...\")\n",
    "      total_files = sum([len(files) for r, d, files in os.walk(base_path)])\n",
    "      processed_files = 0\n",
    "\n",
    "      for class_name in os.listdir(base_path):\n",
    "          class_path = os.path.join(base_path, class_name)\n",
    "          if os.path.isdir(class_path):\n",
    "              print(f\"\\nProcessing class: {class_name}\")\n",
    "\n",
    "              for filename in os.listdir(class_path):\n",
    "                  if filename.endswith('.txt'):\n",
    "                      file_path = os.path.join(class_path, filename)\n",
    "                      try:\n",
    "                          with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                              text = f.read()\n",
    "                              texts.append(text)\n",
    "                              labels.append(class_name)\n",
    "\n",
    "                          processed_files += 1\n",
    "                          if processed_files % 100 == 0:\n",
    "                              print(f\"Processed {processed_files}/{total_files} files\")\n",
    "\n",
    "                      except Exception as e:\n",
    "                          print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "      print(f\"\\nCompleted loading {processed_files} files from {len(set(labels))} classes\")\n",
    "      return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: LLaMA Embedding Function\n",
    "def get_llama_embedding(text, tokenizer, model, device):\n",
    "    \"\"\"Get embeddings from LLaMA model\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                      padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Main Training Function\n",
    "def train_model(base_path):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    texts, labels = load_arabic_dataset(base_path)\n",
    "\n",
    "    # Preprocess texts\n",
    "    print(\"Preprocessing texts...\")\n",
    "    processed_texts = [preprocess_arabic_text(text) for text in texts]\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Load LLaMA model and tokenizer\n",
    "    print(\"Loading LLaMA model...\")\n",
    "    MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    batch_size = 32\n",
    "\n",
    "    for i in range(0, len(processed_texts), batch_size):\n",
    "        batch_texts = processed_texts[i:i + batch_size]\n",
    "        batch_embeddings = [get_llama_embedding(text, tokenizer, model, device)\n",
    "                          for text in batch_texts]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        print(f\"Processed {i + len(batch_texts)}/{len(processed_texts)} texts\")\n",
    "\n",
    "    X = np.array(embeddings)\n",
    "    y = encoded_labels\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Train XGBoost classifier\n",
    "    print(\"Training XGBoost classifier...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(label_encoder.classes_),\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Print classification report\n",
    "    report = classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=label_encoder.classes_,\n",
    "        digits=4\n",
    "    )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Save models and label encoder\n",
    "    print(\"Saving models...\")\n",
    "    xgb_model.save_model(\"llama_xgboost_model.json\")\n",
    "    np.save(\"label_encoder_classes.npy\", label_encoder.classes_)\n",
    "\n",
    "    return xgb_model, label_encoder, tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Prediction Function\n",
    "def predict_new_text(text, xgb_model, tokenizer, llama_model, label_encoder, device):\n",
    "    \"\"\"Predict class for new text\"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_arabic_text(text)\n",
    "\n",
    "    # Get embedding\n",
    "    embedding = get_llama_embedding(processed_text, tokenizer, llama_model, device)\n",
    "\n",
    "    # Reshape for prediction\n",
    "    embedding = embedding.reshape(1, -1)\n",
    "\n",
    "    # Predict\n",
    "    prediction = xgb_model.predict(embedding)\n",
    "    probabilities = xgb_model.predict_proba(embedding)\n",
    "\n",
    "    # Get class name and probability\n",
    "    predicted_class = label_encoder.inverse_transform(prediction)[0]\n",
    "    confidence = np.max(probabilities)\n",
    "\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Starting to load dataset...\n",
      "\n",
      "Processing class: الغرفة الإجتماعية\n",
      "Processed 100/32325 files\n",
      "Processed 200/32325 files\n",
      "Processed 300/32325 files\n",
      "Processed 400/32325 files\n",
      "Processed 500/32325 files\n",
      "Processed 600/32325 files\n",
      "Processed 700/32325 files\n",
      "Processed 800/32325 files\n",
      "Processed 900/32325 files\n",
      "Processed 1000/32325 files\n",
      "Processed 1100/32325 files\n",
      "Processed 1200/32325 files\n",
      "Processed 1300/32325 files\n",
      "Processed 1400/32325 files\n",
      "Processed 1500/32325 files\n",
      "Processed 1600/32325 files\n",
      "Processed 1700/32325 files\n",
      "Processed 1800/32325 files\n",
      "Processed 1900/32325 files\n",
      "Processed 2000/32325 files\n",
      "Processed 2100/32325 files\n",
      "Processed 2200/32325 files\n",
      "Processed 2300/32325 files\n",
      "Processed 2400/32325 files\n",
      "Processed 2500/32325 files\n",
      "Processed 2600/32325 files\n",
      "Processed 2700/32325 files\n",
      "Processed 2800/32325 files\n",
      "Processed 2900/32325 files\n",
      "Processed 3000/32325 files\n",
      "Processed 3100/32325 files\n",
      "Processed 3200/32325 files\n",
      "Processed 3300/32325 files\n",
      "Processed 3400/32325 files\n",
      "Processed 3500/32325 files\n",
      "Processed 3600/32325 files\n",
      "Processed 3700/32325 files\n",
      "Processed 3800/32325 files\n",
      "Processed 3900/32325 files\n",
      "Processed 4000/32325 files\n",
      "Processed 4100/32325 files\n",
      "Processed 4200/32325 files\n",
      "Processed 4300/32325 files\n",
      "Processed 4400/32325 files\n",
      "Processed 4500/32325 files\n",
      "Processed 4600/32325 files\n",
      "Processed 4700/32325 files\n",
      "Processed 4800/32325 files\n",
      "\n",
      "Processing class: الغرفة الإدارية\n",
      "Processed 4900/32325 files\n",
      "Processed 5000/32325 files\n",
      "Processed 5100/32325 files\n",
      "Processed 5200/32325 files\n",
      "Processed 5300/32325 files\n",
      "Processed 5400/32325 files\n",
      "Processed 5500/32325 files\n",
      "Processed 5600/32325 files\n",
      "Processed 5700/32325 files\n",
      "Processed 5800/32325 files\n",
      "Processed 5900/32325 files\n",
      "Processed 6000/32325 files\n",
      "Processed 6100/32325 files\n",
      "Processed 6200/32325 files\n",
      "Processed 6300/32325 files\n",
      "Processed 6400/32325 files\n",
      "Processed 6500/32325 files\n",
      "Processed 6600/32325 files\n",
      "Processed 6700/32325 files\n",
      "Processed 6800/32325 files\n",
      "Processed 6900/32325 files\n",
      "Processed 7000/32325 files\n",
      "Processed 7100/32325 files\n",
      "Processed 7200/32325 files\n",
      "Processed 7300/32325 files\n",
      "Processed 7400/32325 files\n",
      "Processed 7500/32325 files\n",
      "Processed 7600/32325 files\n",
      "Processed 7700/32325 files\n",
      "Processed 7800/32325 files\n",
      "Processed 7900/32325 files\n",
      "Processed 8000/32325 files\n",
      "Processed 8100/32325 files\n",
      "Processed 8200/32325 files\n",
      "Processed 8300/32325 files\n",
      "Processed 8400/32325 files\n",
      "Processed 8500/32325 files\n",
      "Processed 8600/32325 files\n",
      "Processed 8700/32325 files\n",
      "Processed 8800/32325 files\n",
      "Processed 8900/32325 files\n",
      "Processed 9000/32325 files\n",
      "Processed 9100/32325 files\n",
      "Processed 9200/32325 files\n",
      "Processed 9300/32325 files\n",
      "Processed 9400/32325 files\n",
      "Processed 9500/32325 files\n",
      "Processed 9600/32325 files\n",
      "Processed 9700/32325 files\n",
      "Processed 9800/32325 files\n",
      "Processed 9900/32325 files\n",
      "Processed 10000/32325 files\n",
      "\n",
      "Processing class: الغرفة التجارية\n",
      "Processed 10100/32325 files\n",
      "Processed 10200/32325 files\n",
      "Processed 10300/32325 files\n",
      "Processed 10400/32325 files\n",
      "Processed 10500/32325 files\n",
      "Processed 10600/32325 files\n",
      "Processed 10700/32325 files\n",
      "Processed 10800/32325 files\n",
      "Processed 10900/32325 files\n",
      "Processed 11000/32325 files\n",
      "Processed 11100/32325 files\n",
      "Processed 11200/32325 files\n",
      "Processed 11300/32325 files\n",
      "Processed 11400/32325 files\n",
      "Processed 11500/32325 files\n",
      "Processed 11600/32325 files\n",
      "Processed 11700/32325 files\n",
      "Processed 11800/32325 files\n",
      "Processed 11900/32325 files\n",
      "Processed 12000/32325 files\n",
      "Processed 12100/32325 files\n",
      "Processed 12200/32325 files\n",
      "Processed 12300/32325 files\n",
      "Processed 12400/32325 files\n",
      "Processed 12500/32325 files\n",
      "Processed 12600/32325 files\n",
      "Processed 12700/32325 files\n",
      "Processed 12800/32325 files\n",
      "Processed 12900/32325 files\n",
      "Processed 13000/32325 files\n",
      "Processed 13100/32325 files\n",
      "Processed 13200/32325 files\n",
      "Processed 13300/32325 files\n",
      "Processed 13400/32325 files\n",
      "Processed 13500/32325 files\n",
      "Processed 13600/32325 files\n",
      "Processed 13700/32325 files\n",
      "Processed 13800/32325 files\n",
      "\n",
      "Processing class: الغرفة الجنائية\n",
      "Processed 13900/32325 files\n",
      "Processed 14000/32325 files\n",
      "Processed 14100/32325 files\n",
      "Processed 14200/32325 files\n",
      "Processed 14300/32325 files\n",
      "Processed 14400/32325 files\n",
      "Processed 14500/32325 files\n",
      "Processed 14600/32325 files\n",
      "Processed 14700/32325 files\n",
      "Processed 14800/32325 files\n",
      "Processed 14900/32325 files\n",
      "Processed 15000/32325 files\n",
      "Processed 15100/32325 files\n",
      "Processed 15200/32325 files\n",
      "Processed 15300/32325 files\n",
      "Processed 15400/32325 files\n",
      "Processed 15500/32325 files\n",
      "Processed 15600/32325 files\n",
      "Processed 15700/32325 files\n",
      "Processed 15800/32325 files\n",
      "Processed 15900/32325 files\n",
      "Processed 16000/32325 files\n",
      "Processed 16100/32325 files\n",
      "Processed 16200/32325 files\n",
      "Processed 16300/32325 files\n",
      "Processed 16400/32325 files\n",
      "Processed 16500/32325 files\n",
      "Processed 16600/32325 files\n",
      "Processed 16700/32325 files\n",
      "Processed 16800/32325 files\n",
      "Processed 16900/32325 files\n",
      "Processed 17000/32325 files\n",
      "Processed 17100/32325 files\n",
      "Processed 17200/32325 files\n",
      "Processed 17300/32325 files\n",
      "Processed 17400/32325 files\n",
      "Processed 17500/32325 files\n",
      "Processed 17600/32325 files\n",
      "Processed 17700/32325 files\n",
      "Processed 17800/32325 files\n",
      "Processed 17900/32325 files\n",
      "Processed 18000/32325 files\n",
      "Processed 18100/32325 files\n",
      "Processed 18200/32325 files\n",
      "Processed 18300/32325 files\n",
      "Processed 18400/32325 files\n",
      "Processed 18500/32325 files\n",
      "Processed 18600/32325 files\n",
      "Processed 18700/32325 files\n",
      "Processed 18800/32325 files\n",
      "Processed 18900/32325 files\n",
      "Processed 19000/32325 files\n",
      "Processed 19100/32325 files\n",
      "Processed 19200/32325 files\n",
      "Processed 19300/32325 files\n",
      "Processed 19400/32325 files\n",
      "\n",
      "Processing class: الغرفة العقارية\n",
      "Processed 19500/32325 files\n",
      "Processed 19600/32325 files\n",
      "Processed 19700/32325 files\n",
      "Processed 19800/32325 files\n",
      "Processed 19900/32325 files\n",
      "Processed 20000/32325 files\n",
      "Processed 20100/32325 files\n",
      "Processed 20200/32325 files\n",
      "Processed 20300/32325 files\n",
      "Processed 20400/32325 files\n",
      "Processed 20500/32325 files\n",
      "Processed 20600/32325 files\n",
      "Processed 20700/32325 files\n",
      "Processed 20800/32325 files\n",
      "Processed 20900/32325 files\n",
      "Processed 21000/32325 files\n",
      "Processed 21100/32325 files\n",
      "Processed 21200/32325 files\n",
      "Processed 21300/32325 files\n",
      "Processed 21400/32325 files\n",
      "Processed 21500/32325 files\n",
      "Processed 21600/32325 files\n",
      "Processed 21700/32325 files\n",
      "Processed 21800/32325 files\n",
      "Processed 21900/32325 files\n",
      "Processed 22000/32325 files\n",
      "Processed 22100/32325 files\n",
      "Processed 22200/32325 files\n",
      "Processed 22300/32325 files\n",
      "Processed 22400/32325 files\n",
      "Processed 22500/32325 files\n",
      "Processed 22600/32325 files\n",
      "Processed 22700/32325 files\n",
      "Processed 22800/32325 files\n",
      "Processed 22900/32325 files\n",
      "\n",
      "Processing class: الغرفة المدنية\n",
      "Processed 23000/32325 files\n",
      "Processed 23100/32325 files\n",
      "Processed 23200/32325 files\n",
      "Processed 23300/32325 files\n",
      "Processed 23400/32325 files\n",
      "Processed 23500/32325 files\n",
      "Processed 23600/32325 files\n",
      "Processed 23700/32325 files\n",
      "Processed 23800/32325 files\n",
      "Processed 23900/32325 files\n",
      "Processed 24000/32325 files\n",
      "Processed 24100/32325 files\n",
      "Processed 24200/32325 files\n",
      "Processed 24300/32325 files\n",
      "Processed 24400/32325 files\n",
      "Processed 24500/32325 files\n",
      "Processed 24600/32325 files\n",
      "Processed 24700/32325 files\n",
      "Processed 24800/32325 files\n",
      "Processed 24900/32325 files\n",
      "Processed 25000/32325 files\n",
      "Processed 25100/32325 files\n",
      "Processed 25200/32325 files\n",
      "Processed 25300/32325 files\n",
      "Processed 25400/32325 files\n",
      "Processed 25500/32325 files\n",
      "Processed 25600/32325 files\n",
      "Processed 25700/32325 files\n",
      "Processed 25800/32325 files\n",
      "Processed 25900/32325 files\n",
      "Processed 26000/32325 files\n",
      "Processed 26100/32325 files\n",
      "Processed 26200/32325 files\n",
      "Processed 26300/32325 files\n",
      "Processed 26400/32325 files\n",
      "Processed 26500/32325 files\n",
      "Processed 26600/32325 files\n",
      "Processed 26700/32325 files\n",
      "Processed 26800/32325 files\n",
      "Processed 26900/32325 files\n",
      "Processed 27000/32325 files\n",
      "Processed 27100/32325 files\n",
      "Processed 27200/32325 files\n",
      "Processed 27300/32325 files\n",
      "\n",
      "Processing class: غرفة الأحوال الشخصية و الميراث\n",
      "Processed 27400/32325 files\n",
      "Processed 27500/32325 files\n",
      "Processed 27600/32325 files\n",
      "Processed 27700/32325 files\n",
      "Processed 27800/32325 files\n",
      "Processed 27900/32325 files\n",
      "Processed 28000/32325 files\n",
      "Processed 28100/32325 files\n",
      "Processed 28200/32325 files\n",
      "Processed 28300/32325 files\n",
      "Processed 28400/32325 files\n",
      "Processed 28500/32325 files\n",
      "Processed 28600/32325 files\n",
      "Processed 28700/32325 files\n",
      "Processed 28800/32325 files\n",
      "Processed 28900/32325 files\n",
      "Processed 29000/32325 files\n",
      "Processed 29100/32325 files\n",
      "Processed 29200/32325 files\n",
      "Processed 29300/32325 files\n",
      "Processed 29400/32325 files\n",
      "Processed 29500/32325 files\n",
      "Processed 29600/32325 files\n",
      "Processed 29700/32325 files\n",
      "Processed 29800/32325 files\n",
      "Processed 29900/32325 files\n",
      "Processed 30000/32325 files\n",
      "Processed 30100/32325 files\n",
      "Processed 30200/32325 files\n",
      "Processed 30300/32325 files\n",
      "Processed 30400/32325 files\n",
      "Processed 30500/32325 files\n",
      "Processed 30600/32325 files\n",
      "Processed 30700/32325 files\n",
      "Processed 30800/32325 files\n",
      "Processed 30900/32325 files\n",
      "Processed 31000/32325 files\n",
      "Processed 31100/32325 files\n",
      "Processed 31200/32325 files\n",
      "Processed 31300/32325 files\n",
      "Processed 31400/32325 files\n",
      "Processed 31500/32325 files\n",
      "Processed 31600/32325 files\n",
      "Processed 31700/32325 files\n",
      "Processed 31800/32325 files\n",
      "Processed 31900/32325 files\n",
      "Processed 32000/32325 files\n",
      "Processed 32100/32325 files\n",
      "Processed 32200/32325 files\n",
      "Processed 32300/32325 files\n",
      "\n",
      "Completed loading 32325 files from 7 classes\n",
      "Preprocessing texts...\n",
      "Loading LLaMA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad99a0ca4e145ca9d433c1e6b7cbe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77250b9df4b945f7845874fe0daf7e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d398e52a7c2043d7a3c2cce070fd061a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd05f77e218f4e02a73adbf277d25223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33628cd00f49473f8aafbdbe32bee479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 7: Run Training\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your dataset path\n",
    "    base_path =\"C:\\\\Users\\\\PC USER\\\\OneDrive\\\\Bureau\\\\corpusFinal\"\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    xgb_model, label_encoder, tokenizer, llama_model = train_model(base_path)\n",
    "\n",
    "    # Example prediction\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Test with a sample text\n",
    "    sample_text = \"Your Arabic text here\"\n",
    "    predicted_class, confidence = predict_new_text(\n",
    "        sample_text,\n",
    "        xgb_model,\n",
    "        tokenizer,\n",
    "        llama_model,\n",
    "        label_encoder,\n",
    "        device\n",
    "    )\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
